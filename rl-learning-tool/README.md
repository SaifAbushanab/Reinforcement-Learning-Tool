# ğŸ¤– Interactive RL Learning Tool

An interactive web-based platform for learning and experimenting with Reinforcement Learning algorithms.

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red)

## ğŸš€ Installation

```bash
# Clone the repository
git clone https://github.com/SaifAbushanab/Reinforcement-Learning-Tool.git
cd Reinforcement-Learning-Tool/rl-learning-tool

# Install dependencies
pip install -r requirements.txt

# Run the app
streamlit run app.py
```

Then open http://localhost:8501 in your browser.

## ğŸ® Environments

| Environment | Description |
|-------------|-------------|
| **GridWorld** | Navigate from start to goal |
| **FrozenLake** | Reach goal avoiding holes (deterministic) |

## ğŸ“Š Algorithms

| Algorithm | Type | Description |
|-----------|------|-------------|
| **Policy Iteration** | Model-Based | Evaluate + improve loop (includes policy evaluation) |
| **Value Iteration** | Model-Based | Direct optimal V computation |
| **Monte Carlo** | Model-Free | Learn from complete episode returns |
| **TD(0)** | Model-Free | One-step bootstrapping |
| **n-step TD** | Model-Free | Generalized TD/MC bridge |

## âš™ï¸ Parameters

- **Î³ (gamma)**: Discount factor - how much to value future rewards
- **Î± (alpha)**: Learning rate - how fast to update estimates
- **Îµ (epsilon)**: Exploration rate - probability of random action
- **n**: Steps to look ahead (for n-step TD)

## ğŸ“ Project Structure

```
rl-learning-tool/
â”œâ”€â”€ app.py                 # Main Streamlit app
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ environments/          # Environment implementations
â”‚   â”œâ”€â”€ gridworld.py
â”‚   â””â”€â”€ frozenlake.py
â””â”€â”€ algorithms/            # RL algorithms
    â”œâ”€â”€ policy_iteration.py
    â”œâ”€â”€ value_iteration.py
    â”œâ”€â”€ monte_carlo.py
    â”œâ”€â”€ td0.py
    â””â”€â”€ n_step_td.py
```

## ğŸ“– Features

- **Interactive Visualization**: See value functions and policies update in real-time
- **Inference Mode**: Watch the trained agent navigate the environment
- **Convergence Plots**: Track algorithm convergence over iterations
- **Parameter Tuning**: Adjust Î³, Î±, Îµ, and episodes via sliders

## License

MIT
